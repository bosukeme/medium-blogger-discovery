{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPotential Medium, Substack and Blogger extraction\\n\\n- We search twitter for tags and hashtags replated to different types of blogs\\n- We run a search at 1am every day and process all the tweets from the previous day\\n- Parse through the hashtags and get a new list of potential hashtags that should be added for (blog, medium and substack). Each platform should have their list of search terms\\n  and hashtags\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Potential Medium, Substack and Blogger extraction\n",
    "\n",
    "- We search twitter for tags and hashtags replated to different types of blogs (Medium, Substack and General Blogs)\n",
    "- We run a search at 1am every day and process all the tweets from the previous day\n",
    "- Create individual mongo collections for Medium, Substack and Blogs) and save the content in there after each run\n",
    "- ** The target being that 8am everyday when the marketing team start their day, there is fresh content there for them to get \n",
    "  stuck into.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "from newspaper import Article\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Import library for blog discovery pipeline\n",
    "import twitter_blogger_discovery_funcs as blog_discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1 - Get the date for today and 7 days ago\n",
    "\"\"\"\n",
    "today = datetime.now() \n",
    "yesterday = datetime.now() - timedelta(1)\n",
    "last_week = datetime.now() - timedelta(7)\n",
    "today_date = datetime.strftime(today, '%Y-%m-%d')\n",
    "yesterday_date = datetime.strftime(yesterday, '%Y-%m-%d')\n",
    "last_week_date = datetime.strftime(last_week, '%Y-%m-%d')\n",
    "\n",
    "num_tweets = 5000 # The maximum number of tweets that would be extracted in any given run\n",
    "num_posts = 100 # This means that for now, for each content type we will only get getting the top 50 and bottom 50 tweets from the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Medium': ['medium post', 'medium article', 'medium blog', 'medium.com'], 'Substack': ['substack'], 'Blog': ['blog post', 'article', 'blog']}\n",
      "We are now getting the tweets from yesterday related to blogs/articles\n",
      "Medium\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "\n",
      "Medium content extracted in 50.937175035476685 seconds\n",
      "Substack\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "\n",
      "Substack content extracted in 6.470767974853516 seconds\n",
      "Blog\n",
      "\n",
      "Blog content extracted in 272.8710241317749 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup the platform search terms\n",
    "\"\"\"\n",
    "medium_search_list = ['medium post', 'medium article', 'medium blog', 'medium.com']\n",
    "blog_search_list = ['blog post', 'article', 'blog']\n",
    "substack_search_list = ['substack']\n",
    "\n",
    "search_type_list = ['Medium', 'Substack', 'Blog']\n",
    "search_terms_list = [medium_search_list, substack_search_list, blog_search_list]\n",
    "\n",
    "content_type_search_dict = dict(zip(search_type_list, search_terms_list))\n",
    "print(content_type_search_dict)\n",
    "\n",
    "\"\"\"\n",
    "Get the latest extract of content\n",
    "- In future iterations we may want to start excluding anyone that has been pinged more than 3 times\n",
    "\"\"\"\n",
    "content_tweet_dict = blog_discovery.get_latest_article_tweets(content_type_search_dict, num_tweets, num_posts, yesterday_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUkeme - Now save the tweet details into a mongo collection for each of them (medium, substack and blogs)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ukeme - Now save the tweet details into a mongo collection for each of them (medium, substack and blogs)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
