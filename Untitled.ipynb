{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These functions are for the pipeline to identify latest blog/substack/medium tweets\n",
    "** Note that for now, for all cases we are filtering out posts that are not in english\n",
    "\"\"\"\n",
    "\n",
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "from newspaper import Article\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "nest_asyncio.apply()\n",
    "\n",
    "MONGO_URL = MongoClient('mongodb+srv://bloverse:b1XNYDtSQNEv5cAn@bloverse-production.fbt75.mongodb.net/blovids?retryWrites=true&w=majority')\n",
    "db = MONGO_URL.medium_blogger_discovery\n",
    "\n",
    "\n",
    "today = datetime.now() \n",
    "yesterday = datetime.now() - timedelta(1)\n",
    "last_week = datetime.now() - timedelta(7)\n",
    "today_date = datetime.strftime(today, '%Y-%m-%d')\n",
    "yesterday_date = datetime.strftime(yesterday, '%Y-%m-%d')\n",
    "last_week_date = datetime.strftime(last_week, '%Y-%m-%d')\n",
    "\n",
    "num_tweets = 5000 # The maximum number of tweets that would be extracted in any given run\n",
    "num_posts = 100 # This means that for now, for each content type we will only get getting the top 50 and bottom 50 tweets from the extraction\n",
    "\n",
    "\n",
    "\n",
    "medium_search_list = ['medium post', 'medium article', 'medium blog', 'medium.com']\n",
    "blog_search_list = ['blog post', 'article', 'blog']\n",
    "substack_search_list = ['substack']\n",
    "\n",
    "search_type_list = ['Medium', 'Substack', 'Blog']\n",
    "search_terms_list = [medium_search_list, substack_search_list, blog_search_list]\n",
    "\n",
    "content_type_search_dict = dict(zip(search_type_list, search_terms_list))\n",
    "\n",
    "\n",
    "def available_columns():\n",
    "    return twint.output.panda.Tweets_df.columns\n",
    "\n",
    "\n",
    "def twint_to_pandas(columns):\n",
    "    return twint.output.panda.Tweets_df[columns]\n",
    "\n",
    "\n",
    "def get_followings(username):\n",
    "\n",
    "    c = twint.Config()\n",
    "    c.Username = username\n",
    "    c.Pandas = True\n",
    "\n",
    "    twint.run.Following(c)\n",
    "    list_of_followings = twint.storage.panda.Follow_df\n",
    "\n",
    "    return list_of_followings['following'][username]\n",
    "\n",
    "\n",
    "def get_latest_tweets_from_handle(username, num_tweets, date):\n",
    "\n",
    "    c = twint.Config()\n",
    "    c.Username = username\n",
    "    c.Limit = num_tweets\n",
    "    c.Pandas = True\n",
    "    c.Since = date\n",
    "    c.Hide_output = True\n",
    "\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    try:\n",
    "        tweet_df = twint_to_pandas(['id', 'conversation_id', 'date', 'tweet', 'language', 'hashtags', \n",
    "               'username', 'name', 'link', 'urls', 'photos', 'video',\n",
    "               'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'source'])\n",
    "    except:\n",
    "        tweet_df = pd.DataFrame()\n",
    "        \n",
    "    return tweet_df\n",
    "\n",
    "\n",
    "def create_search_strings_from_tweet_df(tweet_df):\n",
    "    search_string_list = []\n",
    "    for i in range(len(tweet_df)):\n",
    "        tweet_text = tweet_df.iloc[i]['tweet']\n",
    "        search_string = \" \".join(tweet_text.split()[0:5])\n",
    "        search_string_list.append(search_string)\n",
    "\n",
    "    tweet_df['Search String'] = search_string_list\n",
    "    return tweet_df\n",
    "\n",
    "\n",
    "def get_tweets_from_search_term(search_term, num_tweets, date):\n",
    "    \"\"\"\n",
    "    This function does a search on twitter and returns the handles of those who\n",
    "    posted the tweets that matched the search term the top 5 most liked tweets for\n",
    "    each search term\n",
    "    \n",
    "    ** Come back to this later and remove the part that only picks \n",
    "    \"\"\"\n",
    "    c = twint.Config()\n",
    "    c.Search = search_term\n",
    "    c.Limit = num_tweets\n",
    "    c.Pandas = True\n",
    "    c.Since = date\n",
    "    c.Hide_output = True\n",
    "\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    try:\n",
    "        search_tweet_df = twint_to_pandas(['id', 'conversation_id', 'date', 'tweet', 'language', 'hashtags', \n",
    "               'username', 'name', 'link', 'urls', 'photos', 'video',\n",
    "               'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'source'])\n",
    "\n",
    "        tweet_w_url_inds = []\n",
    "        for i in range(len(search_tweet_df)):\n",
    "            url = search_tweet_df.iloc[i]['urls']\n",
    "            if len(url) > 0:\n",
    "                tweet_w_url_inds.append(i)\n",
    "\n",
    "        search_tweet_df_final = search_tweet_df.iloc[tweet_w_url_inds]\n",
    "        search_tweet_df_final = search_tweet_df.sort_values(by=['nlikes'], ascending=True) # sort by likes in ascending\n",
    "    except:\n",
    "        search_tweet_df_final = []\n",
    "    \n",
    "    return search_tweet_df_final\n",
    "\n",
    "\n",
    "## Now build a function that filters out all the tweets from a specific date\n",
    "def get_tweets_for_date(tweet_df, date):\n",
    "    \"\"\"\n",
    "    This function takes a date and returns all the tweets from that date\n",
    "    \"\"\"\n",
    "    date_tweet_inds = []\n",
    "    for i in range(len(tweet_df)):\n",
    "        tweet_date = tweet_df['date'].iloc[i][0:10]\n",
    "        if tweet_date == date:\n",
    "            date_tweet_inds.append(i)\n",
    "    \n",
    "    date_tweet_df = tweet_df.iloc[date_tweet_inds]\n",
    "    \n",
    "    return date_tweet_df\n",
    "\n",
    "\n",
    "def cleanup_medium_tweets(tweet_df, num_posts):\n",
    "    \"\"\"\n",
    "    This gets the potential tweets from medium users and then \n",
    "    filters out the ones that dont have a medium link in them\n",
    "    \"\"\"\n",
    "    medium_tweet_inds = []\n",
    "    for i in range(len(tweet_df)):\n",
    "        tweet_url_list = tweet_df['urls'].iloc[i]\n",
    "\n",
    "        if len(tweet_url_list) > 0:\n",
    "            for url in tweet_url_list:\n",
    "                if 'medium.com' in url:\n",
    "                    medium_tweet_inds.append(i)\n",
    "                    \n",
    "    medium_tweet_inds = list(set(medium_tweet_inds))\n",
    "    medium_tweet_df = tweet_df.iloc[medium_tweet_inds]\n",
    "    \n",
    "    ## For now we first want to filter out and only work with tweets that are in english\n",
    "    english_tweet_df = medium_tweet_df[medium_tweet_df['language']=='en']\n",
    "    english_tweet_df = english_tweet_df.sort_values(by=['nlikes'], ascending=False)\n",
    "#     print('Number of english tweets -> %s' % len(english_tweet_df))\n",
    "    \n",
    "    top_english_tweet_df = english_tweet_df.iloc[0:int(num_posts/2)]\n",
    "    bottom_english_tweet_df = english_tweet_df.iloc[-int(num_posts/2):]\n",
    "    medium_tweet_df = pd.concat([top_english_tweet_df, bottom_english_tweet_df])\n",
    "    \n",
    "    # Sort them by number of likes\n",
    "    medium_tweet_df = medium_tweet_df.sort_values(by=['nlikes'], ascending=False)\n",
    "    \n",
    "    return medium_tweet_df\n",
    "\n",
    "\n",
    "def cleanup_substack_tweets(tweet_df, num_posts):\n",
    "    \"\"\"\n",
    "    This gets the potential tweets from medium users and then \n",
    "    filters out the ones that dont have a medium link in them\n",
    "    \"\"\"\n",
    "    substack_tweet_inds = []\n",
    "    for i in range(len(tweet_df)):\n",
    "        tweet_url_list = tweet_df['urls'].iloc[i]\n",
    "\n",
    "        if len(tweet_url_list) > 0:\n",
    "            for url in tweet_url_list:\n",
    "                if 'substack' in url:\n",
    "                    substack_tweet_inds.append(i)\n",
    "                    \n",
    "    substack_tweet_inds = list(set(substack_tweet_inds))\n",
    "    substack_tweet_df = tweet_df.iloc[substack_tweet_inds]\n",
    "    \n",
    "    ## For now we first want to filter out and only work with tweets that are in english\n",
    "    english_tweet_df = substack_tweet_df[substack_tweet_df['language']=='en']\n",
    "    english_tweet_df = english_tweet_df.sort_values(by=['nlikes'], ascending=False)\n",
    "#     print('Number of english tweets -> %s' % len(english_tweet_df))\n",
    "    \n",
    "    top_english_tweet_df = english_tweet_df.iloc[0:int(num_posts/2)]\n",
    "    bottom_english_tweet_df = english_tweet_df.iloc[-int(num_posts/2):]\n",
    "    substack_tweet_df = pd.concat([top_english_tweet_df, bottom_english_tweet_df])\n",
    "    \n",
    "    # Sort them by number of likes\n",
    "    substack_tweet_df = substack_tweet_df.sort_values(by=['nlikes'], ascending=False)\n",
    "    \n",
    "    return substack_tweet_df\n",
    "\n",
    "\n",
    "def cleanup_blog_tweets(tweet_df, num_posts):\n",
    "    \"\"\"\n",
    "    This gets the potential tweets from medium users and then \n",
    "    filters out the ones that dont have a medium link in them\n",
    "    \n",
    "    num_posts: this depicts how many posts we want to extract, we can scale this up as the capacity of the marketing channels get better\n",
    "    \"\"\"\n",
    "    ## First process the tweet df and remove any tweets that dont have links in them\n",
    "    link_tweet_inds = []\n",
    "    for i in range(len(tweet_df)):\n",
    "        tweet_url_list = tweet_df['urls'].iloc[i]\n",
    "        \n",
    "        if len(tweet_url_list) > 0:\n",
    "            link_tweet_inds.append(i)\n",
    "    \n",
    "    link_tweet_inds = list(set(link_tweet_inds))\n",
    "    link_tweet_df = tweet_df.iloc[link_tweet_inds]\n",
    "#     print('Number of link tweets -> %s' % len(link_tweet_df))\n",
    "    \n",
    "    ## For now we first want to filter out and only work with tweets that are in english\n",
    "    english_tweet_df = link_tweet_df[link_tweet_df['language']=='en']\n",
    "    english_tweet_df = english_tweet_df.sort_values(by=['nlikes'], ascending=False)\n",
    "#     print('Number of english tweets -> %s' % len(english_tweet_df))\n",
    "    \n",
    "    top_english_tweet_df = english_tweet_df.iloc[0:int(num_posts/2)]\n",
    "    bottom_english_tweet_df = english_tweet_df.iloc[-int(num_posts/2):]\n",
    "    tweet_df = pd.concat([top_english_tweet_df, bottom_english_tweet_df])\n",
    "#     print('Number of processing tweets -> %s' % len(tweet_df))\n",
    "    ## Now we get only the top 50 and bottom 50, this is all we process for now\n",
    "    \n",
    "    blog_tweet_inds = []\n",
    "    for i in range(len(tweet_df)):\n",
    "#         print(i)\n",
    "        tweet_url_list = tweet_df['urls'].iloc[i]\n",
    "        \n",
    "        # Process the link to check if it passes the parameters of what a blog post should be\n",
    "        try:\n",
    "            article = Article(tweet_url_list[0])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            top_image = article.has_top_image()\n",
    "            text_len = len(article.text)\n",
    "#             print(text_len)\n",
    "\n",
    "            if top_image and (text_len > 1000):\n",
    "                blog_tweet_inds.append(i)\n",
    "                \n",
    "        except Exception as e:\n",
    "#             print(e)\n",
    "            pass\n",
    "\n",
    "\n",
    "    blog_tweet_inds = list(set(blog_tweet_inds))\n",
    "    blog_tweet_df = tweet_df.iloc[blog_tweet_inds]\n",
    "\n",
    "    # Sort them by number of likes\n",
    "    blog_tweet_df = blog_tweet_df.sort_values(by=['nlikes'], ascending=False)\n",
    "    \n",
    "    return blog_tweet_df\n",
    "\n",
    "\n",
    "def process_tweets_from_content(content_type, search_list, num_tweets, num_posts, yesterday_date):\n",
    "    \n",
    "    tweet_df_list = []\n",
    "    for search_term in search_list:\n",
    "        tweet_df = get_tweets_from_search_term(search_term, num_tweets, yesterday_date)\n",
    "        tweet_df_list.append(tweet_df)\n",
    "\n",
    "    search_tweet_df = pd.concat(tweet_df_list)\n",
    "\n",
    "    # Drop duplicates from the output\n",
    "    search_tweet_df.drop_duplicates(subset=['id'], keep=False)\n",
    "\n",
    "    ## Filter out to only get the tweets that were published yesterday\n",
    "    date_tweet_df = get_tweets_for_date(search_tweet_df, yesterday_date)\n",
    "\n",
    "    # Now process the output\n",
    "    if content_type == 'Medium':\n",
    "        content_tweet_df = cleanup_medium_tweets(date_tweet_df, num_posts)\n",
    "\n",
    "    if content_type == 'Substack':\n",
    "        content_tweet_df = cleanup_substack_tweets(date_tweet_df, num_posts)\n",
    "\n",
    "    if content_type == 'Blog':\n",
    "        content_tweet_df = cleanup_blog_tweets(date_tweet_df, num_posts)\n",
    "    print()\n",
    "    \n",
    "    return content_tweet_df\n",
    "\n",
    "def save_to_medium_collection(medium_df):\n",
    "    medium_collection = db.medium_collection\n",
    "    cur = medium_collection.find() \n",
    "    print('We have %s medium entries at the start' % cur.count())\n",
    "    \n",
    "    medium_ids=list(medium_collection.find({},{ \"_id\": 0, \"id\": 1})) \n",
    "    medium_ids=list((val for dic in medium_ids for val in dic.values()))\n",
    "    \n",
    "    for dfs in medium_df.to_dict('records'):\n",
    "        if dfs['id'] not in medium_ids:\n",
    "            medium_collection.insert_one(dfs)\n",
    "            \n",
    "    cur = medium_collection.find() \n",
    "    print('We had %s medium entries at the start' % cur.count())\n",
    "\n",
    "\n",
    "\n",
    "def save_to_blog_collection(blog_df):\n",
    "    blog_collection = db.blog_collection\n",
    "    cur = blog_collection.find() \n",
    "    print('We have %s blog entries at the start' % cur.count())\n",
    "        \n",
    "    blog_ids=list(blog_collection.find({},{ \"_id\": 0, \"id\": 1})) \n",
    "    blog_ids=list((val for dic in blog_ids for val in dic.values()))\n",
    "    \n",
    "    for dfs in blog_df.to_dict('records'):\n",
    "        if dfs['id'] not in blog_ids:\n",
    "            blog_collection.insert_one(dfs)\n",
    "            \n",
    "    cur = blog_collection.find()\n",
    "    print('We had %s blog entries at the start' % cur.count())\n",
    "\n",
    "\n",
    "def save_to_substack_collection(substack_df):\n",
    "    substack_collection = db.substack_collection\n",
    "    cur = substack_collection.find() \n",
    "    print('We have %s substack entries at the start' % cur.count())\n",
    "    \n",
    "    \n",
    "    substack_ids=list(substack_collection.find({},{ \"_id\": 0, \"id\": 1})) \n",
    "    substack_ids=list((val for dic in substack_ids for val in dic.values()))\n",
    "    \n",
    "    for dfs in substack_df.to_dict('records'):\n",
    "        if dfs['id'] not in substack_ids:\n",
    "            substack_collection.insert_one(dfs)\n",
    "            \n",
    "    cur = substack_collection.find()\n",
    "    print('We had %s substack entries at the start' % cur.count())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_latest_article_tweets(content_type_search_dict, num_tweets, num_posts, yesterday_date):\n",
    "    \"\"\"\n",
    "    This function loops through search queries for medium, substack and blogs.\n",
    "    ** Its important to note that for now we are only processing tweets that were in english.\n",
    "    ** This pipeline is to run at 1am everyday and process all the tweets from the previous day so that \n",
    "       by the following morning the guys can hit the ground running hard with it. Later on we may decide to\n",
    "       run the pipeline periodically so its more current but a 24 hour lag is definitely not bad at all.\n",
    "    ** The function returns a dictionary containing dataframes of 'num_post' tweet for each content type.\n",
    "    \"\"\"\n",
    "    print('We are now getting the tweets from yesterday related to blogs/articles')\n",
    "    content_tweet_list = []\n",
    "    for content_type in content_type_search_dict:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            print(content_type)\n",
    "            search_list = content_type_search_dict[content_type]\n",
    "        #     print(search_list)\n",
    "            content_tweet_df = process_tweets_from_content(content_type, search_list, num_tweets, num_posts, yesterday_date)\n",
    "            content_tweet_df['content_type'] = content_type\n",
    "            print(content_tweet_df)\n",
    "\n",
    "            substack_df = content_tweet_df[content_tweet_df['content_type'] == 'Substack']\n",
    "            medium_df = content_tweet_df[content_tweet_df['content_type'] == 'Medium']\n",
    "            blog_df = content_tweet_df[content_tweet_df['content_type'] == 'Blog']\n",
    "\n",
    "            save_to_medium_collection(medium_df)\n",
    "            save_to_substack_collection(substack_df)\n",
    "            save_to_blog_collection(blog_df)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now getting the tweets from yesterday related to blogs/articles\n",
      "Medium\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "\n",
      "                      id      conversation_id                 date  \\\n",
      "275  1334791711961219074  1334791711961219074  2020-12-04 10:28:38   \n",
      "285  1334764868113735680  1334764868113735680  2020-12-04 08:41:57   \n",
      "262  1334888150427168770  1334888150427168770  2020-12-04 16:51:50   \n",
      "224  1334882421247012868  1334882421247012868  2020-12-04 16:29:04   \n",
      "333  1334655353112285184  1334655353112285184  2020-12-04 01:26:47   \n",
      "..                   ...                  ...                  ...   \n",
      "213  1334686411509936129  1334686411509936129  2020-12-04 03:30:12   \n",
      "160  1334986140135387138  1334986140135387138  2020-12-04 23:21:13   \n",
      "307  1334711613153308674  1334711613153308674  2020-12-04 05:10:20   \n",
      "202  1334910343466901504  1334910343466901504  2020-12-04 18:20:01   \n",
      "222  1334946751099203584  1334946751099203584  2020-12-04 20:44:42   \n",
      "\n",
      "                                                 tweet language  \\\n",
      "275  A heartfelt note from the Club President üìùüíú  R...       en   \n",
      "285  A new article on #Medium :) - Solving the miss...       en   \n",
      "262  Link to my Medium post, answering FAQs regardi...       en   \n",
      "224  create a custom checkbox with Swift 5 medium a...       en   \n",
      "333  We just published a new article on @Medium | I...       en   \n",
      "..                                                 ...      ...   \n",
      "213  The Dark Side of Attending an Elite College  h...       en   \n",
      "160  Medium Article Views: Long-Term Growth in View...       en   \n",
      "307  Someone added my article on the Minnesota Goof...       en   \n",
      "202  I just wrote this article on Medium inspired b...       en   \n",
      "222  Energy storage is acelerating the decarbonizat...       en   \n",
      "\n",
      "                                              hashtags         username  \\\n",
      "275                [odishafc, amateamamagame, heroisl]         OdishaFC   \n",
      "285                           [medium, roam, roamcult]       CatoMinor3   \n",
      "262         [covid19, vaccines, idtwitter, medtwitter]   RizwanSohailMD   \n",
      "224              [ios, swift, xcode, uiview, checkbox]      hasanalidev   \n",
      "333                                                 []     ITSPmagazine   \n",
      "..                                                 ...              ...   \n",
      "213  [medium, writing, article, author, blog, ivyle...  Blogging__Guide   \n",
      "160  [medium, mediumarticle, views, seo, marketing,...  Blogging__Guide   \n",
      "307                [medium, writers, softball, sports]      MattReicher   \n",
      "202  [byedon, bidenharris2020, biggestloser, dumptr...     hampmktngdoc   \n",
      "222                     [energystorage, energytwitter]    _EnriqueNieto   \n",
      "\n",
      "                     name                                               link  \\\n",
      "275             Odisha FC  https://twitter.com/OdishaFC/status/1334791711...   \n",
      "285             CatoMinor  https://twitter.com/CatoMinor3/status/13347648...   \n",
      "262  M. Rizwan Sohail, MD  https://twitter.com/RizwanSohailMD/status/1334...   \n",
      "224      Hasan Ali ≈ûi≈üeci  https://twitter.com/hasanalidev/status/1334882...   \n",
      "333  ITSPmagazine Podcast  https://twitter.com/ITSPmagazine/status/133465...   \n",
      "..                    ...                                                ...   \n",
      "213        Blogging Guide  https://twitter.com/Blogging__Guide/status/133...   \n",
      "160        Blogging Guide  https://twitter.com/Blogging__Guide/status/133...   \n",
      "307          Matt Reicher  https://twitter.com/MattReicher/status/1334711...   \n",
      "202     Jennifer Friebely  https://twitter.com/hampmktngdoc/status/133491...   \n",
      "222         Enrique Nieto  https://twitter.com/_EnriqueNieto/status/13349...   \n",
      "\n",
      "                                                  urls  \\\n",
      "275              [https://link.medium.com/wV4wxWmaWbb]   \n",
      "285  [https://catominor3.medium.com/in-the-search-f...   \n",
      "262  [https://riz98.medium.com/mrna-vaccines-for-co...   \n",
      "224  [http://link.medium.com/0XbcgFSFWbb, http://gi...   \n",
      "333              [https://link.medium.com/wHmzKqrDVbb]   \n",
      "..                                                 ...   \n",
      "213  [https://medium.com/escaping-the-9-to-5/the-da...   \n",
      "160  [https://medium.com/blogging-guide/medium-arti...   \n",
      "307  [https://medium.com/illumination/the-unique-st...   \n",
      "202              [https://link.medium.com/bc2fJWmNWbb]   \n",
      "222  [https://medium.com/@enriquen.unam/perspective...   \n",
      "\n",
      "                                                photos  video  \\\n",
      "275                                                 []      0   \n",
      "285  [https://pbs.twimg.com/media/EoYJiRRW8AAAk8x.jpg]      1   \n",
      "262                                                 []      0   \n",
      "224  [https://pbs.twimg.com/tweet_video_thumb/EoZzQ...      1   \n",
      "333                                                 []      0   \n",
      "..                                                 ...    ...   \n",
      "213                                                 []      0   \n",
      "160                                                 []      0   \n",
      "307                                                 []      0   \n",
      "202                                                 []      0   \n",
      "222                                                 []      0   \n",
      "\n",
      "                                             thumbnail  retweet  nlikes  \\\n",
      "275                                                       False     146   \n",
      "285    https://pbs.twimg.com/media/EoYJiRRW8AAAk8x.jpg    False      15   \n",
      "262                                                       False      12   \n",
      "224  https://pbs.twimg.com/tweet_video_thumb/EoZzQt...    False      12   \n",
      "333                                                       False       8   \n",
      "..                                                 ...      ...     ...   \n",
      "213                                                       False       0   \n",
      "160                                                       False       0   \n",
      "307                                                       False       0   \n",
      "202                                                       False       0   \n",
      "222                                                       False       0   \n",
      "\n",
      "     nreplies  nretweets source content_type  \n",
      "275         8          3              Medium  \n",
      "285         0          1              Medium  \n",
      "262         0          2              Medium  \n",
      "224         0          0              Medium  \n",
      "333         0          7              Medium  \n",
      "..        ...        ...    ...          ...  \n",
      "213         0          0              Medium  \n",
      "160         0          0              Medium  \n",
      "307         0          0              Medium  \n",
      "202         0          0              Medium  \n",
      "222         0          0              Medium  \n",
      "\n",
      "[100 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:303: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We have %s medium entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 medium entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:313: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We had %s medium entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 100 medium entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:336: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We have %s substack entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 substack entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:347: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We had %s substack entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 0 substack entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:320: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We have %s blog entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 blog entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:330: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We had %s blog entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 0 blog entries at the start\n",
      "Substack\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "\n",
      "                      id      conversation_id                 date  \\\n",
      "688  1334903214483836930  1334903214483836930  2020-12-04 17:51:42   \n",
      "703  1334898791858769920  1334898791858769920  2020-12-04 17:34:07   \n",
      "743  1334881021964259328  1334881021964259328  2020-12-04 16:23:31   \n",
      "588  1334938286075351041  1334938286075351041  2020-12-04 20:11:04   \n",
      "724  1334891093197025283  1334891093197025283  2020-12-04 17:03:32   \n",
      "..                   ...                  ...                  ...   \n",
      "801  1334851020086784000  1334851020086784000  2020-12-04 14:24:18   \n",
      "827  1334823854078894080  1334823854078894080  2020-12-04 12:36:21   \n",
      "851  1334793788238090241  1334793788238090241  2020-12-04 10:36:53   \n",
      "786  1334868631549845504  1334868631549845504  2020-12-04 15:34:17   \n",
      "770  1334874119322521602  1334874119322521602  2020-12-04 15:56:05   \n",
      "\n",
      "                                                 tweet language  \\\n",
      "688  At Substack I'm running with a Tweet that @Com...       en   \n",
      "703  Some news! In January, I‚Äôm starting my own fre...       en   \n",
      "743  New substack out on how calls to \"rvturn to tr...       en   \n",
      "588  \"What does it say about the media landscape wh...       en   \n",
      "724  I wrote about the campy allure of Iyanla Fix M...       en   \n",
      "..                                                 ...      ...   \n",
      "801  We just subscribed to Everything on Substack, ...       en   \n",
      "827  Bookshop Review for Bloggers - Blogging Guide ...       en   \n",
      "851  Highly underrated substack! Subscribe to Guest...       en   \n",
      "786  I just dropped a new substack post make sure y...       en   \n",
      "770  Tired Of The Social Media Rat Race, Journalist...       en   \n",
      "\n",
      "                                              hashtags         username  \\\n",
      "688                                                 []   PeterRQuinones   \n",
      "703                                                 []     FergusMorgan   \n",
      "743                                                 []          apex_pl   \n",
      "588                                                 []          chaykak   \n",
      "724                                                 []      surlybassey   \n",
      "..                                                 ...              ...   \n",
      "801                                                 []        Umvel_inc   \n",
      "827  [bookshop, medium, substack, writer, bookstore...  Blogging__Guide   \n",
      "851                                                 []  LucianoViterale   \n",
      "786                                                 []   AuguryResearch   \n",
      "770                                                 []    oregonhousing   \n",
      "\n",
      "                  name                                               link  \\\n",
      "688  Peter R. Qui√±ones  https://twitter.com/PeterRQuinones/status/1334...   \n",
      "703      Fergus Morgan  https://twitter.com/FergusMorgan/status/133489...   \n",
      "743               Apex  https://twitter.com/apex_pl/status/13348810219...   \n",
      "588        Kyle Chayka  https://twitter.com/chaykak/status/13349382860...   \n",
      "724  Kaitlyn Greenidge  https://twitter.com/surlybassey/status/1334891...   \n",
      "..                 ...                                                ...   \n",
      "801              Umvel  https://twitter.com/Umvel_inc/status/133485102...   \n",
      "827     Blogging Guide  https://twitter.com/Blogging__Guide/status/133...   \n",
      "851                Luc  https://twitter.com/LucianoViterale/status/133...   \n",
      "786             Augury  https://twitter.com/AuguryResearch/status/1334...   \n",
      "770         Tom Cusack  https://twitter.com/oregonhousing/status/13348...   \n",
      "\n",
      "                                                  urls photos  video  \\\n",
      "688  [https://petequinones.substack.com/p/messaging...     []      0   \n",
      "703                [https://thecrushbar.substack.com/]     []      0   \n",
      "743  [https://apexsnotes.substack.com/p/there-is-no...     []      0   \n",
      "588  [https://medianut.substack.com/p/the-atlantic-...     []      0   \n",
      "724  [https://kaitlyngreenidge.substack.com/p/fix-m...     []      0   \n",
      "..                                                 ...    ...    ...   \n",
      "801  [https://everything.substack.com/?utm_medium=w...     []      0   \n",
      "827  [https://bloggingguide.substack.com/p/bookshop...     []      0   \n",
      "851  [https://guestpostjuice.substack.com/?r=7v3ji&...     []      0   \n",
      "786   [https://augury.substack.com/p/3-2-1-augury-fc9]     []      0   \n",
      "770  [https://www.npr.org/2020/12/02/941020719/tire...     []      0   \n",
      "\n",
      "    thumbnail  retweet  nlikes  nreplies  nretweets source content_type  \n",
      "688              False     118        10         24            Substack  \n",
      "703              False     117         2         38            Substack  \n",
      "743              False      29         5          5            Substack  \n",
      "588              False      19         1          0            Substack  \n",
      "724              False      19         1          3            Substack  \n",
      "..        ...      ...     ...       ...        ...    ...          ...  \n",
      "801              False       0         0          0            Substack  \n",
      "827              False       0         0          0            Substack  \n",
      "851              False       0         0          0            Substack  \n",
      "786              False       0         0          0            Substack  \n",
      "770              False       0         0          0            Substack  \n",
      "\n",
      "[100 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:303: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We have %s medium entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100 medium entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:313: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We had %s medium entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 100 medium entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:336: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We have %s substack entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 substack entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:347: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We had %s substack entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 100 substack entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:320: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We have %s blog entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 blog entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:330: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We had %s blog entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 0 blog entries at the start\n",
      "Blog\n",
      "\n",
      "                       id      conversation_id                 date  \\\n",
      "4687  1334971996602503169  1334971996602503169  2020-12-04 22:25:01   \n",
      "4454  1334983260682805253  1334983245650391040  2020-12-04 23:09:46   \n",
      "4646  1334973884978892802  1334973884978892802  2020-12-04 22:32:31   \n",
      "4483  1334981905083719680  1334981905083719680  2020-12-04 23:04:23   \n",
      "4927  1334960927695728642  1334960927695728642  2020-12-04 21:41:02   \n",
      "...                   ...                  ...                  ...   \n",
      "4994  1334957772333576198  1334957772333576198  2020-12-04 21:28:29   \n",
      "5008  1334956901327630336  1334956901327630336  2020-12-04 21:25:02   \n",
      "4981  1334958265298382849  1334958265298382849  2020-12-04 21:30:27   \n",
      "4978  1334958358445486080  1334958358445486080  2020-12-04 21:30:49   \n",
      "5035  1334954983272423424  1334954983272423424  2020-12-04 21:17:24   \n",
      "\n",
      "                                                  tweet language  \\\n",
      "4687  #YouTube announced it will begin to warn users...       en   \n",
      "4454  That‚Äôs for two reasons: first, I couldn‚Äôt poss...       en   \n",
      "4646  Homeboy Industries would like to thank the @Ra...       en   \n",
      "4483  New blog post: \"Artistic craft beyond rebellio...       en   \n",
      "4927  üì¢Happy to announce the launch of my new blog  ...       en   \n",
      "...                                                 ...      ...   \n",
      "4994  This major blog post my partner created consid...       en   \n",
      "5008  Do you have a travel lover in your life? Check...       en   \n",
      "4981  Order WEAK IS THE NEW STRONG TODAY! View book ...       en   \n",
      "4978  Ultimate Guide to create Great Blog Post Title...       en   \n",
      "5035  Check out this new Microsoft Teams blog post -...       en   \n",
      "\n",
      "                         hashtags      username  \\\n",
      "4687                    [youtube]    EpochTimes   \n",
      "4454                           []      emuehlbe   \n",
      "4646              [givingtuesday]    HomeboyInd   \n",
      "4483                           []      even_kei   \n",
      "4927    [domains, outbound, blog]      ysolanky   \n",
      "...                           ...           ...   \n",
      "4994                           []   mlmleadrush   \n",
      "5008                           []     FlyPilota   \n",
      "4981            [missionalliving]    toddlollar   \n",
      "4978                   [blogging]  amarpatel001   \n",
      "5035  [teamhable, microsoftteams]  MrPaulDredge   \n",
      "\n",
      "                                            name  \\\n",
      "4687                             The Epoch Times   \n",
      "4454                           Ellen Muehlberger   \n",
      "4646                          Homeboy Industries   \n",
      "4483          Marina Ayano: Blog with Zonelets!!   \n",
      "4927                                Yogi Solanki   \n",
      "...                                          ...   \n",
      "4994                           ‚≠êÔ∏èMLM Lead Rush‚≠êÔ∏è   \n",
      "5008                                      Pilota   \n",
      "4981  Todd Lollar, Weak Is The New Strong author   \n",
      "4978                                  Amar Patel   \n",
      "5035                                 Paul Dredge   \n",
      "\n",
      "                                                   link  \\\n",
      "4687  https://twitter.com/EpochTimes/status/13349719...   \n",
      "4454  https://twitter.com/emuehlbe/status/1334983260...   \n",
      "4646  https://twitter.com/HomeboyInd/status/13349738...   \n",
      "4483  https://twitter.com/even_kei/status/1334981905...   \n",
      "4927  https://twitter.com/ysolanky/status/1334960927...   \n",
      "...                                                 ...   \n",
      "4994  https://twitter.com/mlmleadrush/status/1334957...   \n",
      "5008  https://twitter.com/FlyPilota/status/133495690...   \n",
      "4981  https://twitter.com/toddlollar/status/13349582...   \n",
      "4978  https://twitter.com/amarpatel001/status/133495...   \n",
      "5035  https://twitter.com/MrPaulDredge/status/133495...   \n",
      "\n",
      "                                                   urls  \\\n",
      "4687  [https://www.theepochtimes.com/youtube-to-issu...   \n",
      "4454  [https://archeothoughts.wordpress.com/2020/07/...   \n",
      "4646                          [https://buff.ly/3g8VZ9g]   \n",
      "4483  [https://marinakittaka.com/posts/2020-12-04-Ar...   \n",
      "4927                       [http://OutboundDomains.com]   \n",
      "...                                                 ...   \n",
      "4994  [http://blog.workwithjameshicks.com/how-to-bec...   \n",
      "5008                         [http://ow.ly/WJXU50CCY24]   \n",
      "4981                             [https://is.gd/I34BST]   \n",
      "4978                       [https://wpknol.com/?p=2337]   \n",
      "5035  [https://techcommunity.microsoft.com/t5/micros...   \n",
      "\n",
      "                                                 photos  video  \\\n",
      "4687                                                 []      0   \n",
      "4454                                                 []      0   \n",
      "4646  [https://pbs.twimg.com/media/EobHrFMWEAE7kLv.jpg]      1   \n",
      "4483  [https://pbs.twimg.com/media/EobO9zQXIAIKr5Z.png]      1   \n",
      "4927  [https://pbs.twimg.com/media/Eoa7yAhVoAEniTi.jpg]      1   \n",
      "...                                                 ...    ...   \n",
      "4994                                                 []      0   \n",
      "5008                                                 []      0   \n",
      "4981                                                 []      0   \n",
      "4978                                                 []      0   \n",
      "5035                                                 []      0   \n",
      "\n",
      "                                            thumbnail  retweet  nlikes  \\\n",
      "4687                                                     False     128   \n",
      "4454                                                     False      67   \n",
      "4646  https://pbs.twimg.com/media/EobHrFMWEAE7kLv.jpg    False      53   \n",
      "4483  https://pbs.twimg.com/media/EobO9zQXIAIKr5Z.png    False      41   \n",
      "4927  https://pbs.twimg.com/media/Eoa7yAhVoAEniTi.jpg    False      34   \n",
      "...                                               ...      ...     ...   \n",
      "4994                                                     False       0   \n",
      "5008                                                     False       0   \n",
      "4981                                                     False       0   \n",
      "4978                                                     False       0   \n",
      "5035                                                     False       0   \n",
      "\n",
      "      nreplies  nretweets source content_type  \n",
      "4687        56         81                Blog  \n",
      "4454         1          0                Blog  \n",
      "4646         1          5                Blog  \n",
      "4483         7         19                Blog  \n",
      "4927         7          1                Blog  \n",
      "...        ...        ...    ...          ...  \n",
      "4994         0          0                Blog  \n",
      "5008         0          0                Blog  \n",
      "4981         0          0                Blog  \n",
      "4978         0          0                Blog  \n",
      "5035         0          0                Blog  \n",
      "\n",
      "[74 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:303: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We have %s medium entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100 medium entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:313: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We had %s medium entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 100 medium entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:336: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We have %s substack entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100 substack entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:347: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We had %s substack entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 100 substack entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:320: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We have %s blog entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 blog entries at the start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a7fae32b2ede>:330: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print('We had %s blog entries at the start' % cur.count())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 74 blog entries at the start\n"
     ]
    }
   ],
   "source": [
    "get_latest_article_tweets(content_type_search_dict, num_tweets, num_posts, yesterday_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
